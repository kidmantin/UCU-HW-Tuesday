{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acoustic Model. Please read the following definitions and\n",
    "proceed to additional instructions at the end of the file.\n",
    "\n",
    "You will need to install these packages: g2p-en, torch, torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from g2p_en import G2p\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "from praatio import textgrid\n",
    "from praatio.data_classes.interval_tier import Interval\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_frames(wav):\n",
    "    return torchaudio.compliance.kaldi.mfcc(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LibriSpeech(torch.utils.data.Dataset):\n",
    "    def __init__(self, url='dev-clean'):\n",
    "        super().__init__()\n",
    "        self.librispeech = torchaudio.datasets.LIBRISPEECH('.', url=url, download=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.librispeech)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        wav, sr, text, speaker_id, chapter_id, utterance_id = self.librispeech[index]\n",
    "        return make_frames(wav), text, (speaker_id, chapter_id, utterance_id) # changed return, if we have those id returning why not use them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=13, subsample_dim=128, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.subsample = nn.Conv1d(input_dim, subsample_dim, 5, stride=4, padding=3)\n",
    "        self.lstm = nn.LSTM(subsample_dim, hidden_dim, batch_first=True, num_layers=3, dropout=0.2)\n",
    "\n",
    "    def subsampled_lengths(self, input_lengths):\n",
    "        # https://github.com/vdumoulin/conv_arithmetic\n",
    "        p, k, s = self.subsample.padding[0], self.subsample.kernel_size[0], self.subsample.stride[0]\n",
    "        o = input_lengths + 2 * p - k\n",
    "        o = torch.floor(o / s + 1)\n",
    "        return o.int()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = self.subsample(x.mT).mT\n",
    "        x = x.relu()\n",
    "        x, _ = self.lstm(x)\n",
    "        return x.relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.g2p = G2p()\n",
    "\n",
    "        # http://www.speech.cs.cmu.edu/cgi-bin/cmudict\n",
    "        self.rdictionary = [\"ε\", # CTC blank\n",
    "                            \" \",\n",
    "                            \"AA0\", \"AA1\", \"AE0\", \"AE1\", \"AH0\", \"AH1\", \"AO0\", \"AO1\", \"AW0\", \"AW1\", \"AY0\", \"AY1\",\n",
    "                            \"B\", \"CH\", \"D\", \"DH\",\n",
    "                            \"EH0\", \"EH1\", \"ER0\", \"ER1\", \"EY0\", \"EY1\",\n",
    "                            \"F\", \"G\", \"HH\",\n",
    "                            \"IH0\", \"IH1\", \"IY0\", \"IY1\",\n",
    "                            \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\",\n",
    "                            \"OW0\", \"OW1\", \"OY0\", \"OY1\",\n",
    "                            \"P\", \"R\", \"S\", \"SH\", \"T\", \"TH\",\n",
    "                            \"UH0\", \"UH1\", \"UW0\", \"UW1\",\n",
    "                            \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
    "\n",
    "        self.dictionary = {c: i for i, c in enumerate(self.rdictionary)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rdictionary)\n",
    "\n",
    "    def encode(self, text):\n",
    "        labels = [c.replace('2', '0') for c in self.g2p(text) if c != \"'\"]\n",
    "        targets = torch.LongTensor([self.dictionary[phoneme] for phoneme in labels])\n",
    "        return targets\n",
    "    \n",
    "    def to_phonems(self, tokens): # \n",
    "        return [self.rdictionary[token] for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recognizer(nn.Module):\n",
    "    def __init__(self, feat_dim=1024, vocab_size=55+1):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(feat_dim, vocab_size)\n",
    "\n",
    "    def forward(self, features):\n",
    "        features = self.classifier(features)\n",
    "        return features.log_softmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary()\n",
    "encoder = Encoder()\n",
    "recognizer = Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load('lstm_p3_360+500.pt', map_location='cpu')\n",
    "encoder.load_state_dict(ckpt['encoder'])\n",
    "recognizer.load_state_dict(ckpt['recognizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_frames, text, IDTuple = LibriSpeech()[0]\n",
    "phonemes = vocab.encode(text)\n",
    "\n",
    "features = encoder(audio_frames)\n",
    "outputs = recognizer.forward(features) # (T, 55+1)\n",
    "\n",
    "#\n",
    "# Your task is to decode a sequence of vocabulary entries from a sequence of distributions\n",
    "# over vocabulary entries (including blank ε that means \"no output\").\n",
    "#\n",
    "# outputs have dimension (T, V) where V is vocab_size+1 and T is time.\n",
    "# outputs[:,0] is the log probability of a blank emission at every time step.\n",
    "#\n",
    "# Because of the subsampling done by Conv1d the time dimension in the outputs is 4 times smaller\n",
    "# than in the inputs.\n",
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL\n",
      "true phonemes: MIH1STER0 KWIH1LTER0 IH1Z DHAH0 AH0PAA1SAH0L AH1V DHAH0 MIH1DAH0L KLAE1SAH0Z AH0ND WIY1 AA1R GLAE1D TUW1 WEH1LKAH0M HHIH1Z GAA1SPAH0L\n",
      "pred phonemes (with deleted silence): MIH1STER0  KRIH1LTTER0  IH1Z DHAH0  AH0PPAA1SAH0AH0LL  AH1V DHAH0 MIH1DAH0LL KLLAE1SAH0Z   AH0ND WIH1RR  GLLAE1D TWWWEH1LKAH0M   HHIH1Z   GAA1SSPPAH0L\n"
     ]
    }
   ],
   "source": [
    "print(f'text: {text}')\n",
    "print(f'true phonemes: {\"\".join(vocab.to_phonems(phonemes))}')\n",
    "print(f'pred phonemes (with deleted silence): {\"\".join([phonem for phonem in vocab.to_phonems(torch.argmax(outputs, dim=1)) if phonem != \"ε\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2703/2703 [40:59<00:00,  1.10it/s]  \n"
     ]
    }
   ],
   "source": [
    "for audio_frames, text, IDTuple in tqdm(LibriSpeech()):\n",
    "    features = encoder(audio_frames)\n",
    "    outputs = recognizer.forward(features)\n",
    "\n",
    "    tg = textgrid.Textgrid(minTimestamp=0, maxTimestamp=audio_frames.size()[0]/100) # length from 0 to length of audio_file in seconds\n",
    "\n",
    "    phones_tier = textgrid.IntervalTier('phonemes', [], 0, audio_frames.size()[0]/100)\n",
    "\n",
    "    intervals = []\n",
    "    decoded_output_tokens = vocab.to_phonems(torch.argmax(outputs, dim=1))\n",
    "    prev_token, prev_start = None, 0\n",
    "\n",
    "    for i, token in enumerate(decoded_output_tokens):\n",
    "        if prev_token != token and prev_token:\n",
    "            intervals.append(Interval(prev_start, i/25 - 0.22, prev_token)) # since output frame is 25 frames per second we divide i by 25\n",
    "            prev_token = token\n",
    "            prev_start = i/25 - 0.22\n",
    "        elif not prev_token:\n",
    "            prev_token = token\n",
    "    if prev_token:\n",
    "        intervals.append(Interval(prev_start, tg.maxTimestamp, prev_token))\n",
    "        \n",
    "    try:\n",
    "        new_phonemes_tier = phones_tier.new(entries=intervals)\n",
    "        tg.addTier(new_phonemes_tier)\n",
    "    except:\n",
    "        print(f'problem with intervals: {intervals[-1]}, id: {IDTuple[0]}-{IDTuple[1]}-{IDTuple[2]}')\n",
    "        intervals = intervals[:-1]\n",
    "        new_phonemes_tier = phones_tier.new(entries=intervals)\n",
    "        tg.addTier(new_phonemes_tier)\n",
    "    finally:\n",
    "        tg.save(f'textgrids/{IDTuple[0]}-{IDTuple[1]}-{IDTuple[2]}.TextGrid',\n",
    "                includeBlankSpaces=True,\n",
    "                format='long_textgrid',\n",
    "                reportingMode='error')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fullds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
